---
title: 2026
layout: home-with-socials
nav_order: 9
parent: Google Summer Of Code
grand_parent: Development
---

# Ideas Page for Google Summer of Code 2026
{: .no_toc }

We are thrilled to share our carefully curated project ideas for this year's Google Summer of Code.

## General Information
{: .no_toc }

These ideas are just some topics we came up with, where currently nobody is working on. However, Catrobat is a project with a wide range of possibilities and we're aware of our blindspots: So let's live the spirit of Open Source and come up with improvements (e.g., new features, extensions, ...) that are related to the project and in which you're interested in. We do have many senior contributors who would be happy to mentor such a project. Don't be shy and check out the last point on the list: Your idea!

## AI tools policy
{: .no_toc }

You may use AI tools as much as you like (brainstorming, code generation, refactoring, tests, documentation, review). We believe this can significantly improve quality and learning when used thoughtfully. What matters is the outcome: correctness, maintainability, Clean Code and tests. You must be able to explain and justify your implementation and tests.

## General Knowledge Prerequisites for all Projects
{: .no_toc }

- Usage of Git and GitHub
- Software testing (e.g., test doubles) and test-driven development
- Kotlin, Java, or Flutter for Android ideas (depending on target project)
- Swift for iOS ideas
- Also please check that you have the proper hardware for the development (e.g., an Android/iOS smartphone for testing some of the projects, Mac for iOS development etc)

## Idea Overview
{: .no_toc }

1. TOC
{:toc}

## Project Descriptions
{: .no_toc }

### Multiplayer, IoT, and Home Assistant support via two MQTT bricks

90, 175 or 350 hours  
{: .label .label-blue }

{: .highlight  }
> **Required Skills**: Kotlin, Android-Development, Agile Development, Test Driven Development, Clean Code<br>
> **Possible Mentors**: Wolfgang Slany, XXXXX <br>
> **Expected Outcome**: Two new bricks for sending and receiving MQTT messages<br>
> **Difficulty level**: Advanced 

Implement MQTT support for the Catrobat language in Catroid (Pocket Code and its flavors) so Catrobat programs can publish/receive network messages for multiplayer games, IoT, and Home Assistant setups. Add two new Catrobat bricks (activated via app settings):
- Broadcast … via MQTT on channel …
- When you receive a message via MQTT on channel … store it in …

Broker/TLS/auth/client-id are configured globally in app settings, and connect/subscribe happen lazily to avoid boilerplate (wildcard channels should store channel+message as JSON). Provide example projects for two multiplayer architectures:
- one phone acting as a local hub (extending the existing Bluetooth multiplayer variables concept) and
- a dedicated hub over local Wi-Fi (e.g., Mosquitto) coordinating multiple players. Also provide a Home Assistant-oriented topic convention under catrobat/... enabling wall-tablet dashboards and home automations (Tuya/Smart Life and other Home Assistant-supported hardware).

Ensure reliability (reconnect/offline handling) and strong automated tests for core logic (especially topic-filter matching and message routing). 

To apply for this project idea, please complete <a target="_blank" href="https://docs.google.com/document/d/1jw87hf8iq5DStlKOhT9yvQP7trsnzgZ6jYcH99BxBMs/edit?usp=sharing">this entry task</a> and include the links to the requested demo artifact in your application.

<hr>

### Pocket Paint Flutter

350 Hours  
{: .label .label-blue }

{: .highlight  }
> **Required Skills**: Flutter, Dart, Android-Development, Agile Development <br>
> **Possible Mentors**: Abdulbaki Celebi, Mario Kaurin, Julia Herold, Thorsten Bandel <br>
> **Expected Outcome**: Features from Kotlin/Java version of Paintroid ported to new Flutter-based version<br>
> **Difficulty level**: Medium to advanced 

The developer should have knowledge of Flutter. Develop and implement missing tools in Flutter that exist in our old Android app built with Android Native.

<hr>

### AI Mentor for PocketCode Students
350 Hours  
{: .label .label-blue }

{: .highlight  }
> **Required Skills**: Kotlin, Python, JavaScript, Android AI and ML Tools, Android-Development, Agile Development, Test Driven Development, Clean Code<br>
> **Possible Mentors**: Paul Spiesberger, Wolfgang Slany <br>
> **Expected Outcome**: An integrated proof of concept AI mentor within PocketCode <br>
> **Difficulty level**: Advanced

AI is now capable of sophisticated programming and can automate many coding tasks. More importantly, it excels at explaining code to students, making learning more engaging and accessible. Our goal is to integrate our AI-powered mentor, developed during GSoC 2025, into PocketCode. This mentor understands a student’s programming context and provides real-time guidance to enhance learning and coding skills. We already have an existing [AI Tutor SDK](https://github.com/Catrobat/catrobat-ai-tutor) and are now ready to integrate it into PocketCode and its derivatives to test and improve students’ experiences with PocketCode + AI.

The AI mentor could:
- Explain programming concepts, ranging from variables and loops to software design patterns and testing strategies
- Suggest code from text prompts, help debug issues and propose project ideas
- Assist with code architecture, naming conventions and writing tests in the Catrobat language
- Explain and translate downloaded projects from other users

You won’t need to implement everything—just focus on the part that excites you most! The Catrobat team will provide the initial prompt and the necessary API access or local LLMs for support.

<hr>

### Awesome Demo Game Project on Marine Biology
350 Hours  
{: .label .label-blue }

{: .highlight  }
> **Required Skills**: Coding Basics<br>
> **Possible Mentors**: Selina Ernst, Wolfgang Slany <br>
> **Expected Outcome**: Catrobat Demo Game on Marine Biology <br>
> **Difficulty level**: Beginner 

Spend the whole GSoC time developing and designing a demo game. This year's focus of Catrobat includes topics around marine biology as an application area, and we have several projects around the general topic. The present project aims at inspiring young people to become aware of topics related to the protection of marine habits by creating related video games of their own. If you have your own original idea about a game around this topic, please feel free to suggest it. Please note that the demo game will be published under Catrobat’s free open source license, and that the game will thus become part of the Catrobat FLOSS project’s source code. Thus, all artwork, sounds, character names etc must be compatible with our licenses, i.e., freely publishable under our licenses, the AGPL version 3 and CC BY-SA 4.0, or under a compatible, possibly even freer license such as CC0.

<hr>

---

### Skeleton-Based Procedural Animation System for Marine Organisms

350 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: Python, C#, Procedural Animation, Skeletal Systems, Blender Scripting, 3D Geometry, Git Version Control, GitHub, Understanding and integration of ML models, Unity, Blender  
> **Possible Mentors**: Nikhil Ranjan Rajhans, Abha Kumari  
> **Expected Outcome**: A reusable skeleton-driven procedural animation framework for marine animals  
> **Difficulty level**: Advanced  
> **Project size**: Large  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/01_skeleton_based_procedural_animation_system/task.html)

Project Description

This project focuses on creating a skeleton-based procedural animation system for marine animals generated via AI or procedural 3D pipelines. Instead of keyframe animations, animal motion (swimming, turning, fleeing, idling) will be generated dynamically using rule-based skeletal deformation and motion constraints.

Animations will be driven by behavior states and environmental conditions, allowing seamless integration with AI behavior engines and ecosystem simulations. The system will be lightweight, reusable, and suitable for real-time educational applications.

<hr>

### Gemini-Powered Ecosystem Narration and Analysis Interface

175 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: Python, C#, LLM Integration, Prompt Engineering, Explainable AI, Simulation Analysis, Git Version Control, Understanding and integration of ML models, Unity  
> **Possible Mentors**: Abha Kumari, Garima Jain, Kumari Deepika  
> **Expected Outcome**: An AI-powered narration and analysis layer for marine ecosystem simulations  
> **Difficulty level**: Average  
> **Project size**: Medium  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/02_gemini_powered_ecosystem_narration/task.html)


Project Description

This project integrates a Gemini-based natural language interface to enhance accessibility and explainability of marine ecosystem simulations. The AI will provide real-time narration, ecosystem summaries, causal explanations, and natural-language spawning of marine organisms and environmental events.

The LLM will act strictly as an interface and explanation layer, translating simulation states into human-readable insights and structured ecosystem modifications, while core logic remains deterministic and transparent.

<hr>

### AI-Driven Dynamic Procedural Map Generation System

175 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: C#, Procedural Generation, AI Simulation Systems, Spatial Data Structures, Noise Functions, Environmental Modeling, Behavior Modeling, Git Version Control, Auth, DBMS, Understanding and integration of ML models, Unity, Blender  
> **Possible Mentors**: Kumari Deepika, Atharva Prashant Joshi  
> **Expected Outcome**: A dynamically evolving coral reef environment guided by AI-driven simulation models  
> **Difficulty level**: Advanced  
> **Project size**: Medium  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/03_ai_driven_dynamic_procedural_map_generation/task.html)


Project Description

This project extends the mARine AR application by combining procedural generation with AI-based environmental intelligence to create marine ecosystems that evolve realistically over time.

Instead of static procedural placement, the environment will be guided by AI models that simulate reef growth, ecological balance, and adaptive behavior. The system will use a seed-based deterministic generator enhanced by AI rules, ensuring synchronized AR experiences across devices while still allowing intelligent variation.

The system should generate:

- AI-guided terrain topology — Terrain shaped using noise functions enhanced by learned patterns from real reef structures
- Intelligent coral and flora placement — AI models determine clustering, competition, and growth patterns to mimic natural ecosystems
- AI-driven environmental motion — Coral sway, particle flow, and micro-movements based on simulated currents and environmental forces
- Ecosystem evolution over time — Coral growth, decay, and adaptation using rule-based AI or lightweight simulation learning

All generations must remain deterministic from a seed, with AI models acting as rule systems that produce identical synchronized environments across devices.

The final system should feel alive, it should be biologically plausible and continuously evolving.

<hr>

### Upgradation of AR-Based Interactive and Procedural Marine Ecosystem Simulation

350 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: C#, Java, Unity, Vuforia SDK, AR Foundation, Firebase, Cloud, Git Version Control, GitHub, REST API, Auth, DBMS, Understanding and integration of ML models, CI/CD, Blender  
> **Possible Mentors**: Krishna Mohan Patel, Himanshu Kumar  
> **Expected Outcome**: A more realistic, scalable, and performance-optimized AR marine ecosystem platform  
> **Difficulty level**: Advanced  
> **Project Size**: Large  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/04_upgradation_ar_based_interactive_simulation/task.html)


Description

This project focuses on upgrading and strengthening the existing AR-based marine ecosystem simulation platform built during the previous GSoC cycle. The aim is to enhance both scientific realism and system scalability, enabling more accurate ecosystem interactions and smoother deployment across mid-range mobile devices.

The upgraded version will improve procedural generation of marine environments (reef, interaction, deep ocean), introduce more advanced ecosystem behaviors (predator-prey cycles, habitat-based movement, climate-driven changes), and implement performance-critical improvements like optimized LOD pipelines, shader efficiency, and streaming-based spawning.

Additionally, this project will expand infrastructure support using Firebase, cloud-ready services, enabling user authentication, progress storage, module sharing, and future backend integration. Optional integration of lightweight ML models can be explored for behavior prediction, adaptive learning, or intelligent content recommendation.

<hr>

### AR Based Human Interaction Enabled Application for Marine Life

175 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: Python, YOLO, MediaPipe, Pose Detection Algorithms, Machine Learning, C#, Unity, Vuforia SDK, AR Foundation, Firebase, Cloud, Git Version Control, GitHub, REST API, Auth, DBMS, Understanding and integration of ML models, CI/CD, Blender  
> **Possible Mentors**: Udit Narayan, Nikhil Ranjan Rajhans  
> **Expected Outcome**: A complete gesture-driven AR marine learning experience  
> **Difficulty level**: Advanced  
> **Project Size**: Medium  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/05_ar_based_human_interaction_enabled_app/task.html)

Description

This project is focused at extending the marine AR learning system by introducing a human-interaction enabled interface, allowing students to interact naturally with marine life using body gestures and pose-based controls.

Instead of relying only on buttons and UI controls, users will be able to perform interactions such as interfering with a creature's movement using hand gestures, triggering behavior events (octopus camouflage/ink defense etc.), interacting with the ecosystem through human presence and activities through the device camera.

The expected goal is to achieve a complete gesture-driven AR marine learning experience that improves immersion, accessibility, and interaction realism.

<hr>

### Extension of Sandbox Toolkit for simplifying the Development of Marine based AR Modules

350 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: Java, C#, Unity, Unity Editor tooling, Vuforia SDK, ScriptableObjects, AR Foundation, Firebase, Cloud, Git Version Control, GitHub, REST API, Auth, DBMS, Understanding and integration of ML models, CI/CD, Blender  
> **Possible Mentors**: Somya Barolia, Shivendra Verma  
> **Expected Outcome**: A reusable Unity-based Marine AR Module Builder  
> **Difficulty level**: Advanced  
> **Project Size**: Large  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/06_extension_sandbox_toolkit_unity/task.html)

Description

This project enhances our existing Marine AR Module Builder to make it significantly easier for educators and developers to create, package, and distribute marine AR modules without deep technical knowledge.

The extension will focus on improved editor tools for drag-drop environment creation, actor placement & configuration workflows, reusable behavior templates (movement, interactions, ecology rules), JSON/script-based module definition support, validation tools (missing assets, invalid scripts, performance warnings), alignment with marine curriculum and optional cloud syncing (module hosting+version control support).

The toolkit will be built as a modular Unity package so it can be reused beyond this project and integrated into other Catrobat AR education initiatives.

<hr>

### Web-Based Sandbox Toolkit for Marine AR Modules

350 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: JavaScript, WebAR (WebXR), Three.js / A-Frame, HTML/CSS, Firebase, REST API, Git/GitHub, Basic Cloud & CI/CD, Blender  
> **Possible Mentors**: Somya Barolia, Shivendra Verma  
> **Expected Outcome**: A lightweight, reusable Web AR Sandbox Toolkit for marine education  
> **Difficulty level**: Advanced  
> **Project Size**: Large  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/07_web_based_sandbox_toolkit/task.html)

Description

This project extends an existing Web-based Marine AR Sandbox Toolkit to simplify the development and deployment of marine-focused Web AR learning modules. The aim is to enable educators and developers to create interactive marine AR experiences directly from a browser without requiring deep AR or 3D programming expertise.

The extension will introduce a dashboard-based module builder where users can visually assemble marine scenes, place 3D marine organisms, configure basic interactions, and publish Web AR modules that run on mobile browsers.

Scope of Work

- Browser-based dashboard for creating and managing marine AR modules
- Drag-and-drop placement of 3D marine assets in AR scenes
- Reusable behavior templates for basic movement and interactions
- JSON-based module definition and export/import support
- Validation checks for missing assets and basic performance limits
- Cloud-hosted module storage and versioning using Firebase

Expected Outcome

- A lightweight, reusable Web AR Sandbox Toolkit for marine education
- Simplified creation and sharing of marine AR modules via URLs
- A foundation for extending Web-based AR learning across marine science topics

<hr>

### AR Rocket Builder & Space Flight Sandbox

350 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: Augmented Reality (ARCore/ARKit), Physics Simulation, Rigid Body Dynamics, Vector Math, Orbital Mechanics Basics, Unity & Flutter 3D Integration, Firebase, Cloud Sync, Git Version Control, REST API, DBMS, CI/CD  
> **Possible Mentors**: Himanshu Kumar, Abhishek Kumar  
> **Expected Outcome**: An AR rocket construction and flight simulator where users build rockets in their real environment and launch them with physics-accurate behavior  
> **Difficulty level**: Advanced  
> **Project size**: Large  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/08_ar_rocket_builder_space_flight/task.html)

Project Description

This project transforms the app into an augmented reality rocket engineering sandbox. Users build rockets on real-world surfaces using AR placement and modular components. The launch simulation applies thrust, drag, gravity, and fuel consumption in real time.

Rockets launch directly from the floor or table in the user’s environment. Failures such as imbalance, insufficient thrust, or structural instability are visualized physically, helping children understand engineering constraints.

Educational overlays explain forces, center of mass, and orbital velocity using visual arrows and motion trails. The system supports multiple gravity presets (Earth, Moon, Mars) and replay tools to analyze trajectories.

The architecture will allow reusable rocket parts, physics presets, and AR experiment modules for future aerospace learning features.

<hr>

### AR Gravity & Planetary Physics Simulator

175 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: Augmented Reality Rendering, Newtonian Physics Simulation, N-Body Systems, Numerical Integration, Real-Time Optimization, 3D Visualization, Flutter, Unity Firebase, Cloud Systems, Git  
> **Possible Mentors**: Abhishek Kumar, Ashwani Kumar Moudgil  
> **Expected Outcome**: An AR gravity sandbox where users create planetary systems in their room and observe real-time orbital mechanics  
> **Difficulty level**: Advanced  
> **Project size**: Medium  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/09_ar_gravity_planetary_physics_simulator/task.html)

Project Description

This project creates a real-time AR gravity sandbox where celestial bodies appear inside the user’s physical environment. Users spawn planets, stars, or asteroids on tables or floors and observe how gravity shapes motion.

The simulation implements Newtonian gravitational models with optimized solvers for mobile performance. Users can walk around their planetary systems, view orbits from different angles, and physically scale the simulation.

Educational visualizations convert invisible forces into arrows, trails, and orbit predictions. Presets include solar systems, binary stars, asteroid collisions, and black hole experiments.

The system is designed as a reusable AR physics engine supporting scalable multi-body simulations for future educational modules.

<hr>

### AR Interactive Physics Playground

350 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: AR Interaction Design, Physics Engines, Collision Systems, Real-Time Rendering, Mobile Optimization, Educational Game Design, Flutter, Unity, Firebase, Git, REST APIs  
> **Possible Mentors**: Shivendra Verma, Himanshu Kumar  
> **Expected Outcome**: A modular AR physics playground that lets children run real-world science experiments in their environment  
> **Difficulty level**: Advanced  
> **Project size**: Large  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/10_ar_interactive_physics_playground/task.html)

Project Description

This project introduces an augmented reality science playground where children run interactive physics experiments in their surroundings. Objects can be placed on real surfaces and manipulated using gestures to test forces, motion, and collisions.

Mini-labs include pendulums, ramps, projectiles, levers, bouncing systems, and gravity experiments. Each experiment includes guided prompts and simplified explanations.

The playground acts as a reusable AR education framework where new experiments can be plugged in easily. It bridges abstract physics concepts with real-world spatial interaction.

<hr>

### Sentiment Analysis of Cephalopods

350 Hours  
{: .label .label-blue }

{: .highlight }
> **Required Skills**: Java, Python, Machine Learning, Deep Learning, Multi-Modal Modelling, Knowledge Distillation, Model Optimization Techniques, Computer Vision, Git Version Control, GitHub, REST API, Authentication  
> **Possible Mentors**: Aryavardhan Sharma, Krishna Mohan Patel, Himanshu Kumar  
> **Expected Outcome**: An open-source multi-modal pipeline for automated cephalopod behavioral sentiment analysis  
> **Difficulty level**: Advanced  
> **Project Size**: Large  
> **Task link**: [View task](https://aiotsonline.github.io/GSOC/projects/2026/11_sentiment_analysis_cephalopods/task.html)

Description

Cephalopods exhibit complex cognition and emotional-like behavioral states (stress, comfort, aggression, curiosity). However, interpreting their state currently requires expert manual observation and lacks scalable tools.

This project aims to develop an open-source pipeline that uses multi-modal data (behavioral video, bioacoustic data) using computer vision and advanced machine learning algorithms to automatically infer and classify behavioral sentiment states along with recording them for further analysis.

The expected outcomes are to contain of:

- Dataset ingestion pipeline + preprocessing scripts
- Multi-modal model baseline (video + optional audio)
- Behavioral feature extractor (movement, postures, color/pattern changes)
- Sentiment label classification system (stress/calm/curious/aggressive/etc.)
- Training + evaluation scripts with metrics
- Documentation + reproducible experiments
- Deployment API, demo dashboard

The project will help researchers, aquaculture facilities, and education/science communities by providing reproducible tools for cephalopod behavioral analysis. The system will be designed with extensibility in mind, supporting future datasets, species, and deployment environments (edge devices compatibility as well).

<hr>

### Your own Project Ideas ...
90, 175 or 350 Hours  
{: .label .label-blue }

{: .highlight  }
> **Required Skills**: Kotlin, Java, Android-Development, iOS-Development, Agile Development <br>
> **Requirement**: self-organized work <br>
> **Difficulty level**: advanced

In the last years we found that you have many great ideas and knowledge! We're aware that there are many ways how to improve performance, reduce memory usage, make our services more stable and of course the code easier to maintain. We're sure you do have ideas how to achieve this, although we may have never heard of this approach before -> that's the great thing about Open Source! And well, that's also the experience we made at last year's GSoC - and we liked it!

Also new features or extensions for iOS and Android are welcome to be introduced to us. Help us to spread coding and Open Source!
